{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "datasets = {\"multiwoz\":\"multiwoz_results\", \"taskmaster\":\"taskmaster_results\", \"abcd\":\"abcd_results\"}\n",
    "for d in datasets:\n",
    "    cmd = f\"python plot.py --eval_type real --save_dir {datasets[d]}\"\n",
    "    !{cmd}\n",
    "    \n",
    "results_dict = {}\n",
    "for d in datasets:\n",
    "    results_dict[d] = defaultdict(lambda: defaultdict(int))\n",
    "condition_names = {\"Prompting\":\"Prompt\", \"DirectedBeamSearch\":\"DBS\", \"CGMH\":\"CGMH\",'Retrieval':'FOP-retrieval','FuturesOfThePast(no-window)':'FOP-guided(no-window)','FuturesOfThePast':'FOP-guided', 'WindowFuturesOfThePast':'WindowFOP', 'WFirst':'$\\mathcal{W}_{first}$', 'FinetunedModel':'Finetuned'}\n",
    "best_scores = {}\n",
    "for dataset in datasets:\n",
    "    best_scores[dataset] = {'precision':-1,'recall':-1,'f1-score':-1}\n",
    "\n",
    "final_strs = {}\n",
    "for dataset in datasets:\n",
    "    result_dir = datasets[dataset]\n",
    "    if result_dir is None:\n",
    "        continue\n",
    "    for metric in ['precision','recall','f1-score']:\n",
    "        if result_dir == \"\":\n",
    "            continue\n",
    "        data_filename = f\"{result_dir}/keywords_{metric}_results_dict.pkl\"\n",
    "        if os.path.exists(data_filename):\n",
    "            tmp_dict = torch.load(data_filename)\n",
    "        else:\n",
    "            continue\n",
    "        for condition in condition_names:\n",
    "            if condition not in tmp_dict:\n",
    "                continue\n",
    "            value = tmp_dict[condition][-1]\n",
    "            assert value[0] == 9 # check it's for 9 keywords\n",
    "            metric_value = value[1][0]\n",
    "            results_dict[dataset][condition][metric] = metric_value\n",
    "            if metric_value > best_scores[dataset][metric]:\n",
    "                best_scores[dataset][metric] = round(metric_value,2)\n",
    "\n",
    "# Printing precision, recall, F1-score results\n",
    "print(\"Precision, recall, F1-score\\n\")\n",
    "for condition in ['WFirst', 'FinetunedModel', 'Prompting','DirectedBeamSearch','CGMH','Retrieval','FuturesOfThePast', 'FuturesOfThePast(no-window)']:\n",
    "    s = condition_names[condition]+\" & \"\n",
    "    all_f1_scores = []\n",
    "    for dataset in datasets:\n",
    "        tmp_condition = condition\n",
    "        if condition == \"FuturesOfThePast\":\n",
    "            tmp_condition = \"WindowFuturesOfThePast\"\n",
    "        elif condition == \"FuturesOfThePast(no-window)\":\n",
    "            tmp_condition = \"FuturesOfThePast\"\n",
    "        if datasets[dataset] is None:\n",
    "            s += \"0.000 & 0.000 & 0.000 & \"\n",
    "        else:\n",
    "            for m in ['precision', 'recall','f1-score']:\n",
    "                metric_value = round(results_dict[dataset][tmp_condition][m],2)\n",
    "                if m == \"f1-score\":\n",
    "                    all_f1_scores.append(metric_value)\n",
    "                if metric_value == best_scores[dataset][m]:\n",
    "                    s += \"\\\\textbf{\"+str(metric_value)+\"} & \"\n",
    "                else:\n",
    "                    s += f\"{metric_value} & \"\n",
    "    average_f1_score = round(np.mean(all_f1_scores),2)\n",
    "    s += f\"{average_f1_score} & \"\n",
    "    s = \" \".join(s.split()[0:-1]).strip()\n",
    "    if condition != \"FuturesOfThePast\":\n",
    "        s+=\"\\\\\\\\\"\n",
    "    if condition in [\"CGMH\", 'FinetunedModel']:\n",
    "        s += \" \\midrule\"\n",
    "    print(s)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-adaptation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
