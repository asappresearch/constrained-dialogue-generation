[DEFAULT]
log_level = debug
seed = 1234
train_task = customer
no_cuda = False

[DATALOADER]
block_size = 1024
stride = 1024

#keywords_dir = ../data/MultiWoz/KEYWORDS
eval_data_file = ../data/MultiWoz/multiwoz_processed.json
dev_data_file = ../data/MultiWoz/multiwoz_processed.json
train_data_file = ../data/MultiWoz/multiwoz_processed.json
cache_file_postfix = multiwoz_data_cache
keywords_dir = ../data/MultiWoz/KEYWORDS
datastore_file = ../data/MultiWoz/DATASTORE
num_examples = -1
kvstore_n_traincontexts = 56778
kvstore_n_testcontexts = 7372
kvstore_n_devcontexts = 7374
include_future = False
future_type = currfuture

[TRAINING]
model_type = gpt2-medium
model_name = GPT2

train_keywords_file_path = ../data/MultiWoz/KEYWORDS/train_conversation-tfidf.pkl
eval_keywords_file_path = ../data/MultiWoz/KEYWORDS/eval_conversation-tfidf.pkl
training_type = regular
output_dir = simple-multiwoz-finetuned-customer
per_gpu_train_batch_size = 1
batch_size = 32
max_steps = -1
num_train_epochs = 5
gradient_accumulation_steps = 1
weight_decay = 0.0
learning_rate = 5e-5
adam_epsilon = 1e-8
warmup_steps = 0
fp16 = True
fp16_opt_level = O2
max_grad_norm = 1.0
logging_steps = 100
additional_tokens_path = ../data/additional_tokens.csv
save_steps = 10000
per_gpu_eval_batch_size = 1
evaluate_all_checkpoints = True
num_eval_examples = 8884
evaluation_metrics = perplexity,bleu-score,bert-score
